{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "oneDChess.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMzNpVdTqdahRZN5scDTdD3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abuah1999/gymfish/blob/master/oneDChess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NyNiMLGgWfn",
        "outputId": "77f5d06a-a80d-43fb-c118-1f5cf6ba44a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.21.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pHiwAeYmgFS-"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import math\n",
        "#import numpy as np\n",
        "\n",
        "class OneDChessEnvironment(gym.Env):\n",
        "    \"\"\"A 1-D Chess environment for OpenAI gym\"\"\"\n",
        "\n",
        "    worth = {\"K\": 1000010, \"R\": 500, \"N\": 300, \"k\": -1000010, \"r\": -500, \"n\": -300, \".\": 0}\n",
        "\n",
        "    def insufficient_material(self):\n",
        "        count = 0\n",
        "        for c in self.board:\n",
        "            if c == \".\":\n",
        "                count += 1\n",
        "        return count == 6\n",
        "\n",
        "    def is_check(self, board):\n",
        "        i = board.index(\"K\")\n",
        "        is_king_check = (i+1 < 8 and board[i+1] == \"k\") or (i-1 > -1 and board[i-1] == \"k\")\n",
        "        is_knight_check = (i+2 < 8 and board[i+2] == \"n\") or (i-2 > -1 and board[i-2] == \"n\")\n",
        "        is_rook_check = False\n",
        "        for d in (1,-1):\n",
        "            j = i + d\n",
        "            while j < 8 and j > -1:\n",
        "                if board[j]==\".\":\n",
        "                    j += d\n",
        "                elif board[j]==\"r\":\n",
        "                    is_rook_check = True\n",
        "                    break\n",
        "                else:\n",
        "                    break         \n",
        "        return is_king_check or is_knight_check or is_rook_check\n",
        "\n",
        "    def append_move(self, moveList, move, board):\n",
        "        put = lambda board, i, p: board[:i] + p + board[i+1:]\n",
        "        new_board = put(board, move[1], board[move[0]])\n",
        "        new_board = put(new_board, move[0], \".\")\n",
        "        if not self.is_check(new_board):\n",
        "            moveList.append(move)\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "    def legal_moves(self):\n",
        "        moves = []\n",
        "        board = self.board\n",
        "        for i in range(len(board)):\n",
        "            if board[i] == \"K\":\n",
        "                if i+1 < 8 and not board[i+1].isupper():\n",
        "                    self.append_move(moves, (i, i+1), board)\n",
        "                if i-1 > -1 and not board[i-1].isupper():\n",
        "                    self.append_move(moves, (i, i-1), board)\n",
        "            if board[i] == \"N\":\n",
        "                if i+2 < 8 and not board[i+2].isupper():\n",
        "                    self.append_move(moves, (i, i+2), board)\n",
        "                if i-2 > -1 and not board[i-2].isupper():\n",
        "                    self.append_move(moves, (i, i-2), board)\n",
        "            if board[i] == \"R\":\n",
        "                for d in (1,-1):\n",
        "                    j = i + d\n",
        "                    while j < 8 and j > -1:\n",
        "                        if board[j]==\".\":\n",
        "                            self.append_move(moves, (i, j), board)\n",
        "                        elif board[j].islower():\n",
        "                            self.append_move(moves, (i, j), board)\n",
        "                            break\n",
        "                        else:\n",
        "                            break\n",
        "                        j += d\n",
        "        return moves\n",
        "\n",
        "    def flip(self):\n",
        "        self.board = self.board[::-1].swapcase()\n",
        "\n",
        "    def _next_observation(self):\n",
        "        return self.observation_map[self.board]\n",
        "\n",
        "    def reset(self):\n",
        "        self.step_stack = []\n",
        "        self.board = \"KNR..rnk\"\n",
        "        self.balance = sum([self.worth[x] for x in self.board])\n",
        "        return self._next_observation()\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.step_stack = []\n",
        "        self.board = \"KNR..rnk\"\n",
        "        self.balance = sum([self.worth[x] for x in self.board])\n",
        "        self.reward_range = (-math.inf, math.inf)\n",
        "        self.observations = [\"........\"]\n",
        "\n",
        "        put = lambda board, i, p: board[:i] + p + board[i+1:]\n",
        "        for p in \"KNRknr\":\n",
        "            new_list = []\n",
        "            for s in self.observations:\n",
        "               for i in range(len(s)):\n",
        "                   if s[i] == \".\":\n",
        "                       new_list.append(put(s, i, p))\n",
        "            self.observations += new_list\n",
        "\n",
        "        self.observation_map = {}\n",
        "\n",
        "        for i, s in enumerate(self.observations):\n",
        "            self.observation_map[s] = i\n",
        "\n",
        "        self.action_space = gym.spaces.Discrete(9)\n",
        "        self.observation_space = gym.spaces.Discrete(len(self.observations))\n",
        "\n",
        "    def step(self, action):\n",
        "        moves = self.legal_moves()\n",
        "        #print(len(moves))\n",
        "        action = action % len(moves)\n",
        "        self._take_action(moves[action])\n",
        "        reward = self.balance\n",
        "        self.flip()\n",
        "        obs = self._next_observation()\n",
        "        self.balance = -self.balance\n",
        "        reward = -100 if len(self.legal_moves())==0 or (self.balance, self.board) in self.step_stack or self.insufficient_material() else reward\n",
        "        done = reward > 1000000 or reward < -1000000 or len(self.legal_moves())==0 or self.insufficient_material() or (self.balance, self.board) in self.step_stack\n",
        "        self.step_stack.append((self.balance, self.board))\n",
        "        return obs, reward, done, {}\n",
        "\n",
        "    def pop(self):\n",
        "        self.step_stack.pop()\n",
        "        self.board = self.step_stack[-1][1]\n",
        "        self.balance = self.step_stack[-1][0]\n",
        "\n",
        "    def _take_action(self, action):\n",
        "        p = self.board[action[0]]\n",
        "        q = self.board[action[1]]\n",
        "        put = lambda board, i, p: board[:i] + p + board[i+1:]\n",
        "        if (q.islower()):\n",
        "            self.balance -= self.worth[q]\n",
        "        self.board = put(self.board, action[1], p)\n",
        "        self.board = put(self.board, action[0], \".\")\n",
        "        \n",
        "\n",
        "    def render(self):\n",
        "        uni_pieces = {'R':'♜', 'N':'♞', 'K':'♚', \n",
        "                  'r':'♖', 'n':'♘','k':'♔','.':'·'}\n",
        "        board = [uni_pieces[x] for x in self.board]\n",
        "        print(\"\".join(board))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from webbrowser import get\n",
        "#from oned_chess import OneDChessEnvironment\n",
        "import numpy as np \n",
        "\n",
        "# 1. Load Environment and Q-table structure\n",
        "env = OneDChessEnvironment()\n",
        "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
        "# env.observation.n, env.action_space.n gives number of states and action in env loaded\n",
        "# 2. Parameters of Q-learning\n",
        "eta = .628\n",
        "gma = .9\n",
        "epis = 500000\n",
        "rev_list = [] # rewards per episode calculate\n",
        "# 3. Q-learning Algorithm\n",
        "for i in range(epis):\n",
        "    # Reset environment\n",
        "    s = env.reset()\n",
        "    rAll = 0\n",
        "    d = False\n",
        "    j = 0\n",
        "    #The Q-Table learning algorithm\n",
        "    while not d:\n",
        "        #env.render()\n",
        "        # Choose action from Q table\n",
        "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(100./(i+1)))\n",
        "        n = len(env.legal_moves())\n",
        "        #Get new state & reward from environment\n",
        "        s1,r,d,_ = env.step(a)\n",
        "        if not d:\n",
        "            a2 = np.argmax(Q[s1,:] + np.random.randn(1,env.action_space.n)*(100./(i+1)))\n",
        "            s2,_,_,_ = env.step(a2)\n",
        "            #Update Q-Table with new knowledge\n",
        "            while a > -1:\n",
        "                Q[s,a] = Q[s,a] + eta*(r + gma*np.max(Q[s2,:]) - Q[s,a])\n",
        "                a -= n\n",
        "            env.pop()\n",
        "        else:\n",
        "            #Update Q-Table with new knowledge\n",
        "            while a > -1:\n",
        "                Q[s,a] = Q[s,a] + eta*(r - Q[s,a])\n",
        "                a -= n\n",
        "        rAll += r\n",
        "        s = s1\n",
        "        #env.render()\n",
        "    rev_list.append(rAll)\n",
        "    #env.render()\n",
        "print(\"Reward Sum on all episodes \" + str(sum(rev_list)/epis))\n",
        "\n",
        "def get_user_input(env):\n",
        "     print(\"enter your move: \")\n",
        "     user_move = str(input())\n",
        "     #user_move = user_move.split()\n",
        "     #user_move = tuple(user_move)\n",
        "     user_move = (int(user_move[0]), int(user_move[1]))\n",
        "     try:\n",
        "         return env.legal_moves().index(user_move)\n",
        "     except:\n",
        "         print(\"please enter a legal move\")\n",
        "         return get_user_input(env)\n",
        "\n",
        "done = False\n",
        "env = OneDChessEnvironment()\n",
        "s = env.reset()\n",
        "user_turn = False\n",
        "\n",
        "while (not done):\n",
        "    if user_turn:\n",
        "        env.render()\n",
        "        user_action = get_user_input(env)\n",
        "        s,_,done,_ = env.step(user_action)\n",
        "    else:\n",
        "        q_action = a = np.argmax(Q[s,:])\n",
        "        s,_,done,_ = env.step(q_action)\n",
        "    user_turn = not user_turn\n",
        "#env.render()\n",
        "if len(env.legal_moves()) == 0 and not env.is_check(env.board):\n",
        "    print(\"draw.\")\n",
        "elif user_turn:\n",
        "    print(\"better luck next time...\")\n",
        "else:\n",
        "    print(\"you won!\")\n"
      ],
      "metadata": {
        "id": "o3xgN0IZgRyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}